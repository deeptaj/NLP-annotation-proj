{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp23/blob/master/AP/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYm2QjD4WPA"
      },
      "source": [
        "BERT for binary or multiclass document classification using the [CLS] token as the document representation; trains a model (on `train.txt`), uses `dev.txt` for early stopping, and evaluates performance on `test.txt`.  Reports test accuracy with 95% confidence intervals.\n",
        "\n",
        "Before executing this notebook on Colab, make sure you're running on cuda (`Runtime > Change runtime type > GPU`) to make use of GPU speedups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXwcKMY44a04",
        "outputId": "b3753cb8-9489-4b50-ce67-6a409126c5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oeSgrDjF4WPC"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "from transformers import AlbertModel, AlbertTokenizer\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F9qWkf0z4WPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5d2cd7-d2e1-48ba-9570-bc016f923080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# If you have your folder of data on your Google drive account, you can connect that here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OcoE7WJQ6XRp"
      },
      "outputs": [],
      "source": [
        "# Change this to the directory with your data\n",
        "directory=\"/content/drive/MyDrive/ap_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-ycKOkx34WPE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c8fcf3-8d20-49c6-c94d-304b26fe843d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BuzjCRrG4WPF"
      },
      "outputs": [],
      "source": [
        "def read_labels(filename):\n",
        "    labels={}\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[2]\n",
        "            if label not in labels:\n",
        "                labels[label]=len(labels)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8XetKMAw4WPF"
      },
      "outputs": [],
      "source": [
        "def read_data(filename, labels, max_data_points=1000):\n",
        "\n",
        "    data = []\n",
        "    data_labels = []\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols = line.split(\"\\t\")\n",
        "            label = cols[2]\n",
        "            text = cols[3]\n",
        "\n",
        "            data.append(text)\n",
        "            data_labels.append(labels[label])\n",
        "\n",
        "\n",
        "    # shuffle the data\n",
        "    tmp = list(zip(data, data_labels))\n",
        "    random.shuffle(tmp)\n",
        "    data, data_labels = zip(*tmp)\n",
        "\n",
        "    if max_data_points is None:\n",
        "        return data, data_labels\n",
        "\n",
        "    return data[:max_data_points], data_labels[:max_data_points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZPfJqVmb4WPH"
      },
      "outputs": [],
      "source": [
        "labels=read_labels(\"%s/train.txt\" % directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O-728X0b4WPI"
      },
      "outputs": [],
      "source": [
        "train_x, train_y=read_data(\"%s/train.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zQOE469I4WPI"
      },
      "outputs": [],
      "source": [
        "dev_x, dev_y=read_data(\"%s/dev.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vMpv6ZV54WPI"
      },
      "outputs": [],
      "source": [
        "test_x, test_y=read_data(\"%s/test.txt\" % directory, labels, max_data_points=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gUKRjWsf4WPJ"
      },
      "outputs": [],
      "source": [
        "#returns accuracy for classifier\n",
        "def evaluate(model, x, y):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(x, y):\n",
        "            y_predictions=model.forward(x)\n",
        "            for idx, y_pred in enumerate(y_predictions):\n",
        "                prediction=torch.argmax(y_pred)\n",
        "                if prediction == y[idx]:\n",
        "                    correct = correct + 1\n",
        "                total= total + 1\n",
        "    return correct/total, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "frq6wqoWfQ31"
      },
      "outputs": [],
      "source": [
        "#returns predictions from classifier and true values of y\n",
        "def predictions(model, x, y):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    tru_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in zip(x, y):\n",
        "            y_predictions = model.forward(batch_x)\n",
        "            pred_labels = torch.argmax(y_predictions, dim=1)\n",
        "            predictions.extend(pred_labels.cpu().numpy())\n",
        "            tru_labels.extend(batch_y.cpu().numpy())\n",
        "    return predictions, tru_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xkUSuG794WPK"
      },
      "outputs": [],
      "source": [
        "#original BERT model\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, params):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = bert_model_name\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=params[\"doLowerCase\"])\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.num_labels = params['num_labels']\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, self.num_labels)\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        b_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
        "                                attention_mask=batch_x[\"attention_mask\"],\n",
        "                                token_type_ids=batch_x.get(\"token_type_ids\", None),\n",
        "                                output_hidden_states=True)\n",
        "        p_output = b_output.pooler_output\n",
        "        lts = self.fc(p_output)\n",
        "        return lts\n",
        "\n",
        "    def get_batches(self, all_x, all_y, batch_size=16, max_toks=510):\n",
        "        x_batches = []\n",
        "        y_batches = []\n",
        "\n",
        "        for i in range(0, len(all_x), batch_size):\n",
        "            x = all_x[i:i + batch_size]\n",
        "            x_batch = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
        "            y_batch = all_y[i:i + batch_size]\n",
        "\n",
        "            x_batches.append(x_batch.to(device))\n",
        "            y_batches.append(torch.LongTensor(y_batch).to(device))\n",
        "\n",
        "        return x_batches, y_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TKbnvjwKKFc6"
      },
      "outputs": [],
      "source": [
        "class ALBERTClassifier(nn.Module):\n",
        "    def __init__(self, albert_model_name, params):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_name = albert_model_name\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"])\n",
        "        self.albert = AlbertModel.from_pretrained(self.model_name)\n",
        "\n",
        "        self.num_labels = params[\"num_labels\"]\n",
        "        self.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
        "\n",
        "    def get_batches(self, all_x, all_y, batch_size=32, max_toks=510):\n",
        "        x_batches = []\n",
        "        y_batches = []\n",
        "        for i in range(0, len(all_x), batch_size):\n",
        "            x = all_x[i:i+batch_size]\n",
        "            x_batch = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_toks)\n",
        "            y_batch = all_y[i : i +batch_size]\n",
        "            x_batches.append(x_batch.to(device))\n",
        "            y_batches.append(torch.LongTensor(y_batch).to(device))\n",
        "        return x_batches, y_batches\n",
        "\n",
        "    def forward(self, batch_x):\n",
        "        alt_output = self.albert(input_ids=batch_x[\"input_ids\"],\n",
        "                                    attention_mask=batch_x[\"attention_mask\"],\n",
        "                                    token_type_ids=batch_x[\"token_type_ids\"],\n",
        "                                    output_hidden_states=True)\n",
        "\n",
        "        albert_hdn_states = alt_output['hidden_states']\n",
        "        output = albert_hdn_states[-1][:, 0, :]\n",
        "        output = self.fc(output)\n",
        "        return output.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7mU_Fdm8L9Xh"
      },
      "outputs": [],
      "source": [
        "#BERT with attention implemented\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.linear = nn.Linear(embed_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, outputs, mask):\n",
        "        s = self.linear(outputs)\n",
        "        s = self.tanh(s).squeeze(2)\n",
        "        s = s.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = self.softmax(s)\n",
        "        vtr = torch.sum(attn_weights.unsqueeze(-1) * outputs, dim=1)\n",
        "        return vtr\n",
        "\n",
        "class BERTClassifierWithAttention(nn.Module):\n",
        "    def __init__(self, bert_model_name, params):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.attention = Attention(self.bert.config.hidden_size)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, params['num_labels'])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "\n",
        "        last_hdn_states = outputs.last_hidden_state\n",
        "        vtr = self.attention(last_hdn_states, attention_mask)\n",
        "\n",
        "        lts = self.fc(vtr)\n",
        "        return lts\n",
        "\n",
        "class AlbertClassifierWithAttention(nn.Module):\n",
        "    def __init__(self, albert_model_name, params):\n",
        "        super().__init__()\n",
        "        self.albert = AlbertModel.from_pretrained(albert_model_name)\n",
        "        self.attention = Attention(self.albert.config.hidden_size)\n",
        "        self.fc = nn.Linear(self.albert.config.hidden_size, params['num_labels'])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        outputs = self.albert(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              token_type_ids=token_type_ids)\n",
        "\n",
        "        last_hdn_states = outputs.last_hidden_state\n",
        "        vtr = self.attention(last_hdn_states, attention_mask)\n",
        "\n",
        "        lts = self.fc(vtr)\n",
        "        return lts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GiiXWwz94WPL"
      },
      "outputs": [],
      "source": [
        "def train(model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=768, doLowerCase=None, learning_rate=1e-5, batch_size=32, num_epochs=10):\n",
        "    if model_name.startswith(\"bert\"):\n",
        "        model_class = BERTClassifier\n",
        "    elif model_name.startswith(\"albert\"):\n",
        "        model_class = AlbertClassifierWithAttention\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model name. Must start with 'bert' or 'albert'.\")\n",
        "\n",
        "    params = {\n",
        "        'num_labels': len(labels),\n",
        "        'doLowerCase': doLowerCase,\n",
        "        'embedding_size': embedding_size\n",
        "    }\n",
        "\n",
        "    model = model_class(model_name, params)\n",
        "    model.to(device)\n",
        "\n",
        "    batch_x, batch_y = model.get_batches(train_x, train_y, batch_size=batch_size)\n",
        "    dev_batch_x, dev_batch_y = model.get_batches(dev_x, dev_y, batch_size=batch_size)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_dev_acc = 0.\n",
        "    patience = 5\n",
        "    best_epoch = 0\n",
        "    total_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            y_pred = model.forward(x)\n",
        "            loss = cross_entropy(y_pred.view(-1, model.num_labels), y.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_accuracy, _ = evaluate(model, dev_batch_x, dev_batch_y)\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
        "            if dev_accuracy > best_dev_acc:\n",
        "                torch.save(model.state_dict(), model_filename)\n",
        "                best_dev_acc = dev_accuracy\n",
        "                best_epoch = epoch\n",
        "        if epoch - best_epoch > patience:\n",
        "            print(\"No improvement in dev accuracy over %s epochs; stopping training\" % patience)\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filename))\n",
        "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9mVrY9yv4WPL"
      },
      "outputs": [],
      "source": [
        "def confidence_intervals(accuracy, n, significance_level):\n",
        "    c_value = (1-significance_level)/2\n",
        "    z = -1 * norm.ppf(c_value)\n",
        "    se = math.sqrt((accuracy * (1-accuracy))/n)\n",
        "    return accuracy - (se * z), accuracy + (se * z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Twbsysmd4WPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc16185-bd18-47fc-cf01-213e0ecb75c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.540\n",
            "Epoch 1, dev accuracy: 0.520\n",
            "Epoch 2, dev accuracy: 0.560\n",
            "Epoch 3, dev accuracy: 0.600\n",
            "Epoch 4, dev accuracy: 0.610\n",
            "Epoch 5, dev accuracy: 0.610\n",
            "Epoch 6, dev accuracy: 0.660\n",
            "Epoch 7, dev accuracy: 0.690\n",
            "Epoch 8, dev accuracy: 0.700\n",
            "Epoch 9, dev accuracy: 0.720\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.720\n",
            "Test accuracy for best dev model: 0.770, 95% CIs: [0.688 0.852]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# bert-base -- slow on laptop; better on Colab\n",
        "bert_model_name = \"bert-base-cased\"\n",
        "model_filename = \"mybert.model\"\n",
        "embedding_size = 768\n",
        "doLowerCase = False\n",
        "\n",
        "model = train(bert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size, doLowerCase)\n",
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
        "accuracy, test_n = evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper = confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#running ALBERT classifier\n",
        "albert_model_name = \"albert-base-v2\"\n",
        "model_filename = \"myalbert.model\"\n",
        "embedding_size = 768\n",
        "doLowerCase = False\n",
        "\n",
        "model = train(albert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size=embedding_size, doLowerCase=doLowerCase)\n",
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
        "accuracy, test_n = evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper = confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQQJQv-V_PPe",
        "outputId": "ba60c537-5393-4fb0-e1fa-32b4aeaf5ada"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, dev accuracy: 0.550\n",
            "Epoch 1, dev accuracy: 0.610\n",
            "Epoch 2, dev accuracy: 0.680\n",
            "Epoch 3, dev accuracy: 0.710\n",
            "Epoch 4, dev accuracy: 0.740\n",
            "Epoch 5, dev accuracy: 0.740\n",
            "Epoch 6, dev accuracy: 0.710\n",
            "Epoch 7, dev accuracy: 0.670\n",
            "Epoch 8, dev accuracy: 0.680\n",
            "Epoch 9, dev accuracy: 0.690\n",
            "\n",
            "Best Performing Model achieves dev accuracy of : 0.740\n",
            "Test accuracy for best dev model: 0.750, 95% CIs: [0.665 0.835]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UAbB5VZArErE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "da7eb703-2347-4e8c-9358-a4f985a80d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m102.4/107.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.4.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.4.0 scikit-optimize-0.10.1\n",
            "Epoch 0, dev accuracy: 0.550\n",
            "Epoch 1, dev accuracy: 0.710\n",
            "Epoch 2, dev accuracy: 0.620\n",
            "Epoch 3, dev accuracy: 0.550\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-f7d631e7975c>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moptimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_initial_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0moptimal_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_num_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     return base_minimize(\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size, space_constraint)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-f7d631e7975c>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     model = train(albert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels,\n\u001b[0m\u001b[1;32m      9\u001b[0m                   \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoLowerCase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoLowerCase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
            "\u001b[0;32m<ipython-input-26-5742c4419f84>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_name, model_filename, train_x, train_y, dev_x, dev_y, labels, embedding_size, doLowerCase, learning_rate, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#baynesian optimization\n",
        "!pip install scikit-optimize\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "\n",
        "def objective(params):\n",
        "    learning_rate, batch_size, num_epochs = params\n",
        "    model = train(albert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels,\n",
        "                  embedding_size=embedding_size, doLowerCase=doLowerCase,\n",
        "                  learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
        "    dev_batch_x, dev_batch_y = model.get_batches(dev_x, dev_y)\n",
        "    dev_accuracy, _ = evaluate(model, dev_batch_x, dev_batch_y)\n",
        "    return -dev_accuracy\n",
        "\n",
        "albert_model_name = \"albert-base-v2\"\n",
        "model_filename = \"myalbert.model\"\n",
        "embedding_size = 768\n",
        "doLowerCase = False\n",
        "\n",
        "ranges = [\n",
        "    Real(1e-6, 1e-4, name='learning_rate'),\n",
        "    Integer(16, 64, name='batch_size'),\n",
        "    Integer(5, 20, name='num_epochs')\n",
        "]\n",
        "\n",
        "optimized = gp_minimize(objective, ranges, n_calls=2, n_initial_points=2)\n",
        "\n",
        "optimal_learning_rate, optimal_batch_size, optimal_num_epochs = optimized.x\n",
        "print(f\"Optimal hyperparameters: learning_rate={optimal_learning_rate}, batch_size={optimal_batch_size}, num_epochs={optimal_num_epochs}\")\n",
        "\n",
        "final_model = train(albert_model_name, model_filename, train_x, train_y, dev_x, dev_y, labels,\n",
        "                    embedding_size=embedding_size, doLowerCase=doLowerCase,\n",
        "                    learning_rate=optimal_learning_rate, batch_size=optimal_batch_size, num_epochs=optimal_num_epochs)\n",
        "\n",
        "test_batch_x, test_batch_y = final_model.get_batches(test_x, test_y)\n",
        "accuracy, test_n = evaluate(final_model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper = confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1znZZ3lfiNl"
      },
      "outputs": [],
      "source": [
        "predictions, true_labels = predictions(model, test_batch_x, test_batch_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9GjI9cXfneR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7076db2a-186a-41d0-bb44-400d66769759"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "np.sum(np.array(predictions) == np.array(true_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyImsx_C4WPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa525c19-1940-448b-b577-9a7d1bb0f54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for best dev model: 0.740, 95% CIs: [0.654 0.826]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_batch_x, test_batch_y = model.get_batches(test_x, test_y)\n",
        "accuracy, test_n=evaluate(model, test_batch_x, test_batch_y)\n",
        "\n",
        "lower, upper=confidence_intervals(accuracy, test_n, .95)\n",
        "print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4wL2Z8w4WPO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "17ab6014-0e27-4e0f-d5f6-76671b31d13c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'randomly selected train dev and test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\"\"\"randomly selected train dev and test\"\"\"\n",
        "#code used to create dev, train, and txt files (from adjuncted csv)\n",
        "#df=pd.read_csv(\"%s/adj.csv\" % directory)\n",
        "#df = pd.read_csv(\"adj.csv\")\n",
        "#df = df.sample(n = 500, replace = False)\n",
        "#train = df[:300]\n",
        "#dev = df[300: 400]\n",
        "#test = df[400:]\n",
        "\n",
        "#train.to_csv(\"train.txt\", sep='\\t', header=False, index=False)\n",
        "#dev.to_csv(\"dev.txt\", sep='\\t', header=False, index=False)\n",
        "#test.to_csv(\"test.txt\", sep='\\t', header=False, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8dMXPcD-c1v"
      },
      "source": [
        "# Backtranslation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuJZm_8POQhA"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def back_translate(text, source_lang='en', target_lang='fr'):\n",
        "    fwd_tokenizer = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}')\n",
        "    fwd_model = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}')\n",
        "\n",
        "    bwd_tokenizer = MarianTokenizer.from_pretrained(f'Helsinki-NLP/opus-mt-{target_lang}-{source_lang}')\n",
        "    bwd_model = MarianMTModel.from_pretrained(f'Helsinki-NLP/opus-mt-{target_lang}-{source_lang}')\n",
        "\n",
        "    translated = fwd_model.generate(**fwd_tokenizer(text, return_tensors=\"pt\", padding=True))\n",
        "    target_text = fwd_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "    back_translated = bwd_model.generate(**bwd_tokenizer(target_text, return_tensors=\"pt\", padding=True))\n",
        "    source_text = bwd_tokenizer.decode(back_translated[0], skip_special_tokens=True)\n",
        "\n",
        "    return source_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV9GVhN17X-J"
      },
      "outputs": [],
      "source": [
        "augmented_text = train['original_text'][:25].apply(lambda x: back_translate(x, src_lang='en', tgt_lang='fr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5SzxLAOHmte"
      },
      "outputs": [],
      "source": [
        "augmented_df = train.iloc[:25]\n",
        "augmented_df['augmented_text'] = augmented_text\n",
        "\n",
        "augmented_df.to_csv(\"augmented_first25.txt\", sep='\\t', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-R4fbEgzYSe"
      },
      "outputs": [],
      "source": [
        "augmented_text2 = train['original_text'][25:50].apply(lambda x: back_translate(x, src_lang='en', tgt_lang='fr'))\n",
        "augmented_df2 = train.iloc[25:50]\n",
        "augmented_df2['augmented_text'] = augmented_text2\n",
        "\n",
        "augmented_df2.to_csv(\"augmented_2.txt\", sep='\\t', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzWKUIjlHrOH"
      },
      "outputs": [],
      "source": [
        "augmented_text3 = train['original_text'][50:75].apply(lambda x: back_translate(x, src_lang='en', tgt_lang='fr'))\n",
        "augmented_df3 = train.iloc[50:75]\n",
        "augmented_df3['augmented_text'] = augmented_text3\n",
        "\n",
        "augmented_df3.to_csv(\"augmented_3.txt\", sep='\\t', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwQek6mEIi7X"
      },
      "outputs": [],
      "source": [
        "augmented_text4 = train['original_text'][75:100].apply(lambda x: back_translate(x, src_lang='en', tgt_lang='fr'))\n",
        "augmented_df4 = train.iloc[75:100]\n",
        "augmented_df4['augmented_text'] = augmented_text4\n",
        "\n",
        "augmented_df4.to_csv(\"augmented_4.txt\", sep='\\t', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiQjM6Vq-3JK"
      },
      "outputs": [],
      "source": [
        "print(train[['original_text', 'augmented_text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtranslation takes ~1 hour to run and produce augmented data."
      ],
      "metadata": {
        "id": "mFebOvXC71SX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A Summary\n",
        "We decided to go with the BERT model for our task because it’s most helpful for picking up surrounding context and assessing humor in text. There are a few changes we implemented to improve our model.\n",
        "\n",
        "Augmented data/Back Translation - We wanted to introduce variation in the data and make it more robust to increase model performance. By doing this the model would ideally better capture nuances in humor leading to more accurate predictions.\n",
        "\n",
        "ALBERT Classifier - We thought that ALBERT's parameter reduction techniques would help improve efficiency and enable better performance with limited labeled data.\n",
        "\n",
        "Bayesian Optimization - We implemented a ski-kit optimizer into our ALBERT classifier to tune hyperparameters and improve accuracy.\n",
        "\n",
        "Layer Adjustments - Instead of using the baseline CLS token representation in the given BERT notebook, we used pooled output and attention mechanisms to aggregate information across all tokens in order to improve our accuracy.\n",
        "\n",
        "Accuracy: 0.740\n",
        "\n",
        "CI: [0.654 0.826]\n"
      ],
      "metadata": {
        "id": "RrO_0XsIzlaS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3arvm6loWtzu"
      },
      "source": [
        "##Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgH5533clVrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "4ec8d110-b517-436d-8cb8-844cbdc2e0bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "conf_matrix = confusion_matrix(test_y, predictions)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANTJ_GOAnpjR"
      },
      "outputs": [],
      "source": [
        "error_cases = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_cases['original_text'] = test_x\n",
        "error_cases['annotated_label'] = test_y\n",
        "error_cases['model_prediction'] = predictions"
      ],
      "metadata": {
        "id": "bv5hZo8fcplt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part_a = error_cases[error_cases['annotated_label'] == 0]\n",
        "error_cases_02 = part_a[part_a['model_prediction'] == 2]"
      ],
      "metadata": {
        "id": "-R9yuVGrc7X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part_one = error_cases[error_cases['annotated_label'] == 2]\n",
        "error_cases_20 = part_a[part_a['model_prediction'] == 0]"
      ],
      "metadata": {
        "id": "CpCLvxxvdYTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(error_cases_20['original_text'])[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "S7okT5Z9kFZE",
        "outputId": "62786782-69d6-440c-b292-5dad4b168249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"My students often ask me, \"\"What is sociology?\"\" And I tell them it\\'s the study of the way in which human beings are shaped by things that they don\\'t see. And they say, \"\"So, how can I be a sociologist? How can I understand those invisible forces?\"\" And I say, \"\"Empathy. Start with empathy. It all begins with empathy. Take yourself out of your shoes, put yourself into the shoes of another person.\"\"Here, I\\'ll give you an example. So I imagine my life if, a hundred years ago, China had been the most powerful nation in the world and they\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation/Discussion"
      ],
      "metadata": {
        "id": "20qFMMohE9Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most common mistaken labels:**\n",
        "\n",
        "The labels of 0 and 2 were often mistaken for each other. For example, we had 10 errors where our model predicted the label was 0 when it was actually 2. This means even though a category was labeled as funny, our model assessed it as not funny. The same situation occurred in the opposite way where our model predicted a text would be funny when it actually had a label as 0. We want to take a closer look at test examples to see why this error is coming up.\n",
        "\n",
        "**Systematic mistakes & biases:**\n",
        "\n",
        "When reading through test predictions, we noticed that in the texts we labeled as not funny but the model predicted as funny, there are niche references to pop culture. In our view, they were attempted jokes or comments that had the intention to be humorous but didn’t really have a punchline. The model seems to favor pop culture references and connects it to humor automatically. This brings up a bias on our end; because all annotators are the same age and in the same environment, we tend to have a pretty similar sense of humor. We aren’t in the age bracket to understand certain older references and therefore may have missed jokes that other people would pick up on and find amusing.\n",
        "\n",
        "Another issue we found is that the model doesn’t pick up on dark humor. There are certain examples where we labeled the text as funny because even though the topic was on the “darker” side, the speaker incorporated humor throughout their speech. However, the model seemed to have zoned in on the serious topic and labeled it as not funny.\n",
        "\n",
        "**Dataset balance:**\n",
        "\n",
        "In our dataset, there is a huge prevalence of \"Not Funny\" data or labels with the value of 0. This negatively impacts our model since our model doesn't have enough data points of other types of data (\"Funny\" and \"Neutral\"). We also classified inputs who didn't fit our general structure of Ted Talks (singing or poems) as 0, increasing the total number of \"Not Funny\" data labels. With data sets like ours, we want to duplicate examples from the minority data values."
      ],
      "metadata": {
        "id": "J2YThwj6ygjd"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}